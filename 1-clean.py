# -*- coding: utf-8 -*-
"""clean.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oXQrDaG5iOU4DPP4UTQ1aeY7O4fMToJD
"""

try:
  from google.colab import drive
  drive.mount('/content/drive')
  drive_path = '/content/drive/My\ Drive/'
except ImportError:
  drive_path = '.'

from glob import glob
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display
import seaborn as sns

dataset_name = 'cse-cic-ids2018'
dataset_file = f'{dataset_name}.zip'
dataset_path = Path(drive_path, f'datasets/registry.opendata.aws/{dataset_name}')
!pwd
print(dataset_path)

"""Create local directory to store the dataset files of https://www.unb.ca/cic/datasets/ids-2018.html"""

! if ! test -r $dataset_name; then mkdir $dataset_name && cp $dataset_path/$dataset_file $dataset_name; fi

!ls -al $dataset_name

! if test -r $dataset_name/$dataset_file; then cd $dataset_name && unzip $dataset_file && rm -f $dataset_file; fi

!ls -al $dataset_name

!ls -alh $dataset_name

!df -h

"""Perform feature selection separately for every data file"""

!cp /content/drive/My\ Drive/Colab\ Notebooks/nd00333/nd00333_capstone/clean/clean.py .
!cat clean.py > /dev/null

import importlib
import clean

importlib.reload(clean)

"""Summarize one of the smaller data sets.

The following observations can be made:

1. 'Flow Byts/s' and 'Flow Pkts/s' columns contain non-numeric values
2. 'Init Fwd Win Byts' and 'Init Bwd Win Byts' contain a negative number '-1'
3. 'Flow IAT Min' amd 'Fwd IAT Min' contain large absolute negative values

The rows with those values in the respective columns will be removed (1. and 2., note that 2. results in a significant decrease in the number of non-Benign flows for a couple of data sets) or replaced (3.) in the the `get_clean_df` function.
"""

df = clean.get_clean_df(f'{dataset_name}/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv', verbose=True)
feature_list = clean.get_feature_list(df, tolerance=0.0001, sample_fraction=0.5)
print(feature_list)
del df

"""A 12 GB machine is unable to keep copies of the largest dataset `Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv` in memory. Therefore some of the low variance and duplicate features found in smaller datasets are removed upfront from the lagest dataset to reduce its size. Moreover the lagest data file contains `extra_features` not present in other data files, and they are therefore removed. Moreover due to a large number (almost 8 millions) samples if the largest data set a sample of 5% is used in the process of feature selection.

Many people (e.g. Frank Harrell https://twitter.com/f2harrell/status/1137012097391312897?lang=en `Feature selection doesn't work in general because it can't find the right variables and distorts statistical properties.  One summary of the evils of stepwise`) claim that no feature selection should be performed. In this case reducing the number of features is necessary due to limited computing resources.

In principle a feature selection should happen on an isolated subset of the data, in order to not involve the test data in any model choices. This approach is not followed strictly here, since another separate test set https://www.unb.ca/cic/datasets/ids-2017.html is used for the final estimation of the model performance.

The features are selected in `get_feature_list` using an addition process, where features are added on-by-one in the order of importance, only if by adding a feature the performance metrics (the macro average of recall across all target classes) increases by a threshold.
"""

columns = []
for dataset_file in sorted(glob(f'{dataset_name}/*.csv')):
  columns_dataset_file = pd.read_csv(f'{dataset_file}', index_col=0, nrows=0).columns.tolist()
  columns_new = set(columns_dataset_file) - set(columns)
  if len(columns_new):
    print(f'New columns in {dataset_file}', columns_new)
    columns.extend(columns_new)

quasi_constant_features = ['Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'CWE Flag Count', 'Fwd Byts/b Avg',
                           'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg']
duplicated_features = ['Subflow Fwd Pkts', 'Subflow Bwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Byts', 'Fwd Seg Size Avg',
                       'Bwd Seg Size Avg', 'SYN Flag Cnt', 'ECE Flag Cnt']
extra_features = ['Src IP', 'Src Port', 'Dst Port', 'Dst IP']

selected_features = {}
for dataset_file in sorted(glob(f'{dataset_name}/*.csv')):
  if dataset_file == f'{dataset_name}/Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv':
    columns_to_remove = quasi_constant_features + duplicated_features + extra_features
    sample_fraction = 0.05
  else:
    columns_to_remove = extra_features
    sample_fraction = 0.5
  columns = pd.read_csv(dataset_file, index_col=0, nrows=0).columns.tolist()
  usecols = []
  for column in columns:
    if column not in columns_to_remove:
      usecols.append(column)    
  df = clean.get_clean_df(dataset_file, usecols=usecols, verbose=True)
  feature_list = clean.get_feature_list(df, tolerance=0.001, sample_fraction=sample_fraction)
  del df
  selected_features[dataset_file] = feature_list

"""Find the union set of selected features across all data files"""

selected_features_common = []
for dataset_file, features_list in sorted(selected_features.items()):
  print(f'Merging features for {dataset_file}', features_list)
  for feature in features_list:
    if feature not in selected_features_common:
      selected_features_common.append(feature)

print(f'Number of selected features {len(selected_features_common)}')
for feature in selected_features_common:
  print(feature)

"""Save selected features data into new csv files"""

dataset_name_clean = dataset_name + '-clean'

!mkdir -p $dataset_name_clean

for dataset_file in sorted(glob(f'{dataset_name}/*.csv')):
  file_name = dataset_file.split('/')[-1]
  df = clean.get_clean_df(dataset_file, usecols=selected_features_common + ['Label'], verbose=False)
  df.to_csv(f'{dataset_name_clean}/{file_name}', index=False)

!ls -al $dataset_name_clean

!ls -alh $dataset_name_clean

!head -2 $dataset_name_clean/*.csv

"""Load all data files into a common dataframe"""

df = pd.concat(map(pd.read_csv, glob(f'{dataset_name_clean}/*.csv')))

with pd.option_context('display.max_rows', None, 'display.max_columns', None):
    display(df.head().transpose())
    display(df.describe().transpose())

size = df.groupby(['target']).size().reset_index(name='count')
display(size)

size['fraction'] = df.groupby(['target']).size().reset_index(name='count').apply(lambda x: x['count'] / df.shape[0], axis=1)
display(size)

size.plot.bar(x='target', y='fraction')

"""Explore correlations between features. In can be noted that there are groups of features close to correlated:

1. 'TotLen Bwd Pkts' and 'Bwd Header Len'
2. 'Bwd Pkt Len Std', 'Bwd Pkt Len Max', 'Pkt Len Std' and 'Pkt Len Max'
3. 'RST Flag Cnt' and 'ECE Flag Cnt'

These correlations, for the non-Benign labels are explored in more details further below, and since the correlation coefficient does not represent a linear relationship, all the above features are kept.
"""

df_corr = df.corr()
fig, ax = plt.subplots(figsize=(15, 15))
sns.heatmap(df_corr, xticklabels=df_corr.columns, yticklabels=df_corr.columns, annot=True, fmt='.1f', ax=ax)

def plot_corr(data, x, y, xlim, ylim):
  import matplotlib.pyplot as plt
  fig, ax = plt.subplots(figsize=(15, 15))
  data.plot.scatter(x=x, y=y, ax=ax)
  ax.set_xlim(xlim)
  ax.set_ylim(ylim)

plot_corr(df[df['target'] != 'Benign'], 'TotLen Bwd Pkts', 'Bwd Header Len', xlim=(-1, 1e5), ylim=(-1, 1e5))

plot_corr(df[df['target'] != 'Benign'], 'Bwd Pkt Len Std', 'Bwd Pkt Len Max', xlim=(-1, 1e3), ylim=(-1, 1e3))

plot_corr(df[df['target'] != 'Benign'], 'Bwd Pkt Len Std', 'Pkt Len Std', xlim=(-1, 1e3), ylim=(-1, 1e3))

plot_corr(df[df['target'] != 'Benign'], 'RST Flag Cnt', 'ECE Flag Cnt', xlim=(-1, 2), ylim=(-1, 2))

"""Save the clean dataset archive"""

dataset_clean_file = f'{dataset_name_clean}.zip'

!rm -f $dataset_clean_file
!zip -r $dataset_clean_file $dataset_name_clean

! /bin/cp -f $dataset_clean_file $dataset_path

!jupyter nbconvert --to html '/content/drive/My Drive/Colab Notebooks/nd00333/clean.ipynb'
